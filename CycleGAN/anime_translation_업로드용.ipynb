{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEyCTvUM4s_u"
      },
      "source": [
        "# Selfie to Anime Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmfJZccX4s_x"
      },
      "source": [
        " **Code 0. Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QDWk1qya4s_x"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj5uq-8Mqf9l",
        "outputId": "fb465882-af59-499d-923f-aa2c03fd0de5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kDgDtn4s_y"
      },
      "source": [
        "**Code 1. Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l--twMXI4s_y"
      },
      "outputs": [],
      "source": [
        "# 이미지 RGB형식으로 변환\n",
        "def to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mLzZYP8V4s_y"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.unaligned = unaligned #unaligned=False이면 비정렬형태 (1:1로 매칭x)\n",
        "        # train 모드일 때는 trainA, trainB에 있는 디렉토리에서 이미지를 불러옵니다.\n",
        "        if mode==\"train\":\n",
        "            # glob 함수로 trainA 디렉토리의 이미지의 목록을 불러옵니다.\n",
        "            self.files_A = sorted(glob.glob(os.path.join(root, \"trainA\") + \"/*.*\"))\n",
        "            self.files_B = sorted(glob.glob(os.path.join(root, \"trainB\") + \"/*.*\"))\n",
        "        else:\n",
        "            self.files_A = sorted(glob.glob(os.path.join(root, \"testA\") + \"/*.*\"))\n",
        "            self.files_B = sorted(glob.glob(os.path.join(root, \"testB\") + \"/*.*\"))\n",
        "\n",
        "    #학습할 이미지를 랜덤으로 불러오기\n",
        "    def __getitem__(self, index):\n",
        "        # index값으로 이미지의목록 중 이미지 하나를 불러옵니다.\n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "        # unaligned 변수로 학습할 Pair를 랜덤으로 고릅니다. (1:1 pair로 매칭되는 형태는 아니므로 아무거나 랜덤으로 고르는 게 가능함)\n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "\n",
        "        # Convert grayscale images to rgb\n",
        "        if image_A.mode != \"RGB\":\n",
        "            image_A = to_rgb(image_A)\n",
        "        if image_B.mode != \"RGB\":\n",
        "            image_B = to_rgb(image_B)\n",
        "\n",
        "        item_A = self.transform(image_A)\n",
        "        item_B = self.transform(image_B)\n",
        "        return {\"A\": item_A, \"B\": item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhtbbolo4s_z"
      },
      "source": [
        "**Code 2. Generator & Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sPVT4IV14s_z"
      },
      "outputs": [],
      "source": [
        "#layer별 가중치 초기화\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1: #Conv layer일 때\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1: #BatchNorm2d layer일 때\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RxyqAIU54s_0"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), #가장 가까운 픽셀 값을 복사해서 padding에 이용함 -> zero padding보다 훨씬 더 자연스러운 이미지를 생성함\n",
        "            nn.Conv2d(in_features, in_features, 3), ##nn.Conv2d()의 인자 -> 첫번째 인자 = input channel수 = filter depth, 두번째 인자 = output channel수 = filter 개수, 세번째 인자 = filter크기 (3x3)\n",
        "            nn.InstanceNorm2d(in_features), #개별 이미지를 정규화\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "        )\n",
        "    # 중요 부분 (이전 layer의 출력값을 단순히 다음 layer에 더해서, gradient vanishing 문제를 해결한 부분) (Residual connection)\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EkcxYYLo4s_0"
      },
      "outputs": [],
      "source": [
        "# ResNet을 적용한 GAN #중요 부분\n",
        "# 이 과정을 거치면 이미지 feature가 추출됨\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_residual_blocks):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        channels = input_shape[0]\n",
        "\n",
        "        # Initial convolution block\n",
        "        out_features = 64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_features, 7),\n",
        "            nn.InstanceNorm2d(out_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        in_features = out_features\n",
        "\n",
        "        # Downsampling -> input image의 특징을 압축 추출하는 부분!! #중요\n",
        "        # 정보를 응축시키고 residual block을 이용해서 최대한 많은 특징을 추출해내는 부분\n",
        "        for _ in range(2):\n",
        "            out_features *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), #stride = 2인 DownSampling을 2번하므로 이미지 크기가 1/4배가 됨\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResidualBlock(out_features)]\n",
        "\n",
        "        # Upsampling -> 이미지의 스타일을 변경(강조)해주는 용도 #중요\n",
        "        # 응축해놨던 정보를 펼치는 부분!\n",
        "        for _ in range(2):\n",
        "            out_features //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2), #이미지 2배 확대\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Output layer -> 최종적으로 추출된 특징으로부터 이미지를 생성하는 부분\n",
        "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SJVtTN9w4s_0"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # Calculate output shape of image discriminator (PatchGAN)\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)] #Down sampling을 통해 출력 image의 크기 줄이기\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        ) #discriminator block 4번 통과해서 이미지 사이즈가 1/16으로 줄어듦 (왜냐면 stride가 2), 그리고 채널수는 discriminator block을 통과할수록 증가\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_2kXWUS4s_1"
      },
      "source": [
        "**Code 3. Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5fw6pzzV4s_1"
      },
      "outputs": [],
      "source": [
        "dataset_name=\"selfie2anime\"\n",
        "channels = 3\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "n_residual_blocks=9\n",
        "lr=0.0002\n",
        "b1=0.5\n",
        "b2=0.999\n",
        "n_epochs=20\n",
        "init_epoch=0\n",
        "decay_epoch=100\n",
        "lambda_cyc=10.0\n",
        "lambda_id=5.0\n",
        "n_cpu=8\n",
        "batch_size=4\n",
        "sample_interval=100\n",
        "checkpoint_interval=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ubLd93UK4s_1"
      },
      "outputs": [],
      "source": [
        "# Create sample and checkpoint directories\n",
        "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
        "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO3X60RE7P31",
        "outputId": "7a5af34e-d7e7-4a45-91ea-911957ed048b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -qq \"../images/selfie2anime/archive.zip\" -d \"../images/selfie2anime/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42AuQv5w6h0M",
        "outputId": "78ea69f1-81d1-4c4a-8d2d-a655df731d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open ../images/selfie2anime/archive.zip, ../images/selfie2anime/archive.zip.zip or ../images/selfie2anime/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rLXj9yia4s_1"
      },
      "outputs": [],
      "source": [
        "# Losses\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss() #!\n",
        "criterion_identity = torch.nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jbOs_YJl4s_1"
      },
      "outputs": [],
      "source": [
        "input_shape = (channels, img_height, img_width)\n",
        "\n",
        "# Initialize generator and discriminator (모델 선언)\n",
        "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "D_A = Discriminator(input_shape)\n",
        "D_B = Discriminator(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kZo4Qv194s_1"
      },
      "outputs": [],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "if cuda:\n",
        "    G_AB = G_AB.cuda()\n",
        "    G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AdA9K29c4s_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c017ce1b-cf7e-4ffa-afba-121c12dfd857"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): ZeroPad2d((1, 0, 1, 0))\n",
              "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Initialize weights\n",
        "G_AB.apply(weights_init_normal)\n",
        "G_BA.apply(weights_init_normal)\n",
        "D_A.apply(weights_init_normal)\n",
        "D_B.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e9UCTk1v4s_2"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PIgTPgU74s_2"
      },
      "outputs": [],
      "source": [
        "# learning rate scheduler\n",
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        #assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Th8Waqn4s_2"
      },
      "outputs": [],
      "source": [
        "# Learning rate update schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o3x15qbD4s_2"
      },
      "outputs": [],
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mgiOUCld4s_2"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yp-6huIF4s_2"
      },
      "outputs": [],
      "source": [
        "# Buffers of previously generated samples\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uWU6owPD4s_2"
      },
      "outputs": [],
      "source": [
        "# Image transformations\n",
        "transforms_ = [\n",
        "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
        "    transforms.RandomCrop((img_height, img_width)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zZPEhHd14s_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cc1894-7421-4967-e9d7-704c048900cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Training data loader\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"/content/drive/MyDrive/archive/\" , transforms_=transforms_, unaligned=True),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        ")\n",
        "# Test data loader\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(\"/content/drive/MyDrive/archive/\", transforms_=transforms_, unaligned=True, mode=\"test\"),\n",
        "    batch_size=5,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sKD2gHoE4s_3"
      },
      "outputs": [],
      "source": [
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    G_AB.eval()\n",
        "    G_BA.eval()\n",
        "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = G_AB(real_A)\n",
        "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "    fake_A = G_BA(real_B)\n",
        "    # Arange images along x-axis\n",
        "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
        "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
        "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
        "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
        "    # Arange images along y-axis\n",
        "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
        "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "teViRatW4s_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ec15e2-eda1-482d-e19c-687be7cd322c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-e0659687f55f>:11: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False) #진짜를 나타내는 label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 20/20] [Batch 852/853] [D loss: 0.088454] [G loss: 3.321859, adv: 0.847585, cycle: 0.176013, identity: 0.142830] ETA: -1 day, 23:54:16.816157"
          ]
        }
      ],
      "source": [
        "# 이 부분이 제일 중요함!!\n",
        "prev_time = time.time()\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # (1) Set model input\n",
        "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "        # (2) Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False) #진짜를 나타내는 label\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False) #가짜를 나타내는 label\n",
        "\n",
        "        # (3) Train Generators\n",
        "\n",
        "        G_AB.train()\n",
        "        G_BA.train()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # (4) Identity loss (L1 loss)\n",
        "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "\n",
        "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "        # Generator는 MSE loss -> L1 loss를 이용\n",
        "        # (5) GAN loss (MSE loss) #중요\n",
        "        fake_B = G_AB(real_A) # B hat\n",
        "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) #생성된 이미지를 판별한 결과와 실제 이미지 label의 차이를 계산해서,\n",
        "                                                        #생성된 이미지가 진짜와 유사하게 보이도록 'Generator'를 훈련시키는 데 사용됨\n",
        "                                                        #D_B가 가짜라고 판단하면, Discriminator는 잘 판단한 거지만 Generator가 잘 생성하지 못한 거여서 이 loss값이 클 것이고,\n",
        "                                                        #Generator를 더 훈련시키는 데에 이용됨\n",
        "        fake_A = G_BA(real_B) # A hat\n",
        "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "        # (6) Cycle loss (L1 loss) #중요\n",
        "        recov_A = G_BA(fake_B) # a hat\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A) #원본 이미지와 복원 이미지의 차이값\n",
        "        recov_B = G_AB(fake_A) # b hat\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B) #원본 이미지와 복원 이미지의 차이값\n",
        "\n",
        "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "        # (7) Total loss\n",
        "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "        #-----------------------------------------------------------------------#\n",
        "        # (8) Train Discriminator A\n",
        "\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # (9) Real loss\n",
        "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "        # (10) Fake loss (on batch of previously generated samples)\n",
        "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "        # (11) Total loss\n",
        "        loss_D_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # (12) Train Discriminator B\n",
        "\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # (13) Real loss\n",
        "        loss_real = criterion_GAN(D_B(real_B), valid) #Discriminator의 능력을 평가 (loss값이 작으면 discriminator가 잘 판별하는 것)(평가에 이용되는 데이터가 실제이미지)\n",
        "\n",
        "        # (14) Fake loss (on batch of previously generated samples)\n",
        "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake) #Discriminator의 능력을 평가 (loss값이 작으면 discriminator가 잘 판별하는 것)(평가에 이용되는 데이터가 가짜이미지)\n",
        "\n",
        "        # (15) Total loss\n",
        "        loss_D_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        loss_D = (loss_D_A + loss_D_B) / 2\n",
        "\n",
        "        # (16) Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = n_epochs * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # (17) Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_GAN.item(),\n",
        "                loss_cycle.item(),\n",
        "                loss_identity.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # (18) If at sample interval save image\n",
        "        if batches_done % sample_interval == 0:\n",
        "            sample_images(batches_done)\n",
        "\n",
        "    # (19) Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "\n",
        "    # (20) Save model checkpoints\n",
        "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
        "        torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_lsh",
      "language": "python",
      "name": "env_lsh"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}